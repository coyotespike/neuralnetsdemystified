{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Proof of Sum Rule\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Proof of (d/dx) [f(x) + g(x)] = (d/dx) f(x) + (d/dx) g(x) from the definition.\n\nWe can use the definition of the derivative:\n\n$\\frac{d}{dx} f(x) =\t\\lim_{x\\to 0} \\frac{f(x+d)-f(x)}{d}$\n\nTherefore, $\\frac{d}{dx} [f(x) + g(x)] =\t\\lim_{x\\to 0} \\frac{[f(x+d)+ g(x+d)] - [f(x)+g(x)]}{d}$\n\nLet's just rearrange this a little: $\\frac{[f(x+d) - f(x) ] + [g(x+d) + g(x)]}{d}$, and then $\\frac{f(x+d) - f(x)}{d} + \\frac{g(x+d) + g(x)}{d}$.\n\nBut this is just our original definition.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Proof of Chain Rule\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Prime notation shows the mechanics clearer, Leibniz notation the concept.\nPrime notation: $(f o g)' (x) = f'(g(x)) \\cdot g'(x)$\n\nAnd from Leibniz: $\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}$\n\nSince we are asking about a rate of change, relative to another rate of change,\nwhich is relative to x, it make sense that any change to x gets amplified as it\nricochets through our little machine.\n\ndy/dx is the result of a function. It is f(x + dx) - f(x) by (x + dx).\nActually, I suppose they are both infinitesimals, but the dy infinitesimal is\nobtained through a function.\n\nThe proof is a straight-up algebraic \"chain\" of reasoning.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Part A\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Remember $\\Delta y = f(a + \\Delta x) - f(x)$, and as $\\Delta x \\rightarrow 0$ we have $\\Delta y/\\Delta x = f'(a)$.\n\nLet's make $\\epsilon$ equal the difference between our [ difference quotient ](https://en.wikipedia.org/wiki/Difference_quotient) and the derivative.\n\nAs $\\Delta x \\rightarrow 0$, we have $\\epsilon = (\\frac{\\Delta y}{\\Delta x} - f'(a)) = f'(a) - f'(a) = 0$.\n\nThis lets us do $\\epsilon = (\\Delta y/\\Delta x - f'(a) \\Rightarrow \\Delta y = f'(a) \\Delta x + \\epsilon \\Delta x$\n\nTo make $\\epsilon$ differentiable, we let it go to 0 when $\\Delta x = 0$.\n$\\Delta y = f'(a) \\Delta x + \\epsilon \\Delta x$ where $\\epsilon \\to 0$ as $\\Delta x \\to 0$.\n\nThis is a little obscurantist, because f'(a) and our difference quotient are exactly\nthe same thing.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Part B\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The stage is now set. Suppose $u = g(x)$ is differentiable at a, and $y = f(u)$ at \nthe same location, but we'll say $b = g(a)$. $f(g(a))$, or $f(b)$.\n\nThen, $\\Delta u = g'(a) \\Delta x + \\epsilon \\Delta x$. And this equals $[g'(a) + \\epsilon](\\Delta x)$.\n\nAnd, $\\Delta y = f'(b) (\\Delta u) + \\epsilon (\\Delta u)$. And, $= [f'(b) + \\epsilon](\\Delta u)$.\n\nSubstituting the first result into the second, we have\n$\\Delta y = [f'(b) + \\epsilon] [g'(a) + \\epsilon](\\Delta x)$.\nNow as $\\Delta x \\to 0$, so does $\\Delta u$, and thus both our $\\epsilon \\to 0$.\nTherefore $\\frac{dy}{dx} = (\\Delta x \\to 0) \\frac{(\\Delta y)}{(\\Delta x)} = (lim_{\\Delta x \\to 0}) [f'(b) + \\epsilon] [g'(a) + \\epsilon] = f'(b) g'(a) = f'(g(a)) g'(a)$.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### This proves the Chain Rule.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We found a way to represent the change in y as the result of the derivative of\na point plus a small change, multiplied by the change in x - to show a change in\ny in reference to an already-established rate.\n\nThis somewhat sneaky characterization let us see more clearly what we could not\nbefore - that the change in y relative to the change in x, when functions are composed,\ndepends on the derivatives of both functions.\n\n$\\epsilon = \\frac{\\Delta y)}{\\Delta x} - \\frac{\\Delta y}{\\Delta x}$\n\n$\\Delta x \\epsilon = \\Delta y - (\\Delta x)[\\frac{(\\Delta y}{(\\Delta x)}]$\n\n$\\Delta y = (\\Delta x)[\\frac{(\\Delta y}{(\\Delta x)}] + (\\Delta x) \\epsilon$\n\n$\\Delta y = (\\Delta x)[\\frac{(\\Delta y}{(\\Delta x)} + \\epsilon]$\n\n$(\\frac{\\Delta y)}{(\\Delta x)} = (\\frac{\\Delta y)}{(\\Delta u)} \\frac{(\\Delta u}{(\\Delta x)}$\n\nBut since we are not using infinitesimals, and we use the Leibniz notation because \nsometimes it shows these relations more clearly, we cannot cancel out $(\\Delta u)$.\nRather, these represent the results of taking the limit, and given any actual\nfunctions f and g, we would have to apply our usual method of adding an h and\ndoing algebra, and so on.\n\nSo we can treat $\\Delta x$ and others above as algebraic quantities, but to actually\nfind the derivative we then must find the limit as $\\Delta x \\to 0$. We can work with them\nalgebraically before taking the limit, but not use them to actually find the derivative.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Sal Khan's Method\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Is, naturally, the easiest.\n\n$\\frac{dy}{dx} = lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x}$ \n\nIf we multiply the last part, we get:\n$lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x} \\frac{\\Delta u}{\\Delta u} = lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta u} \\frac{\\Delta u}{\\Delta x}$ \n\nBut we're allowed to take the limits separately.\n\n$lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta u} \\frac{\\Delta u}{\\Delta x} = lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta u} \\cdot lim_{\\Delta x \\to 0} \\frac{\\Delta u}{\\Delta x}$ \n\nAnd we can change the first x to a u, because as x approaches 0, so does the function of x:\n\n$lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta u} \\frac{\\Delta u}{\\Delta x} = lim_{\\Delta u \\to 0} \\frac{\\Delta y}{\\Delta u} \\cdot lim_{\\Delta x \\to 0} \\frac{\\Delta u}{\\Delta x}$ \n\nAnd naturally that's $\\frac{dy}{du} \\frac{du}{dx}$.\n\n(although so far as I can see, when we say u approaches 0 when x does, we are kind of cheating, because we are saying u is 0 in the first but not in the second).\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Back Propagation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["I found the video's explanation confusing, but the calculus is very simple.\n\nThe chain rule is like using silverware at a fine dining experience: start from the outside and work in.\n\nFor instance $x^2$ can be thought of as $(x)^2$. Take the derivative of the outer function, $2(x)$, then multiply by the derivative of the inner function, which is $x$ with a derivative of $1$: $2(x) * 1 = 2x$.\n\n\\\\[\n\n\\begin{center}\n\\begin{tabular}{ c c }\n Outer & x^4 \\\\ \n     & (x^2)^2 \\\\  \n to & 2 x^2 \\cdot (x^2)' \\\\\n    &  2 \\cdot 2x \\cdot x' \\\\\n Inner & 2 \\cdot 2x \\cdot x = 4x^3\n\\end{tabular}\n\\end{center}\n\n\\\\]\n\nTo take a partial derivative, you take one variable at a time, and treat the other variables like constants. For instance, if you had $2x$, you could split it into $x + x$ and differentiate each separately.\n\nThe partial derivative asks \"how much did each part contribute?\" Here each part contributed $1$.\n\nOur cost function is $J = \\sum \\frac{1}{2}(y - \\hat{y})^2$, and we want to take its derivative so we know how to improve our weights.\n\nWe can add it all up later, so we ignore the summation. The derivative of $\\frac{1}{2}(y - \\hat{y})^2$ is just $ 2 \\* \\frac{1}{2}(y - \\hat{y})$ times the derivative of the inner function.\n\nClearly the 2 and 0.5 multiply out, leaving us with $(y - \\hat{y})$ for the first outer function.\n\nFor the inner function, $y$ is a constant and disappears. If we separate the negative sign like $(-1) \\hat{y}$, it won't disappear but the math is easier.\n\nTo differentiate $\\hat{y}$, we look at the actual function, which is $\\sigma(w2 * x)$, where $x$ is the example that W2 gets. So we differentiate $\\sigma$, then differentiate the inner function. Let's differentiate $\\sigma$ in a second or two.\n\nAt this point our entire derivative is just: $(y - \\hat{y}) \\cdot \\sigma'(w2 * x) \\cdot (w2 * x)'$, where we haven't yet figured out the last term.\n\nThe last term is easy though. $w2$ is our variable, and $x$ is a constant. It is like normal calculus, where you have to differentiate $2x$ or something, and the derivative is just $2$.\n\nHere the derivative is $x$, which is the result of $\\sigma(w1 * x)$, which is more math to do, but we don't have to do any calculus on it, it is just straight multiplication.\n\nWe now have $(y - \\hat{y}) \\cdot \\sigma'(w2 * x) \\cdot x$, and all we have to do is differentiate $\\sigma$.\n\nYou can apply the product rule or the quotient rule, whichever you like.\n\nTo take this derivative, rewrite as $(1 + e^{-z})^{-1}$, and apply the power\nrule to get $-(1 + e^{-z})^{-2}$, but then we must apply the product rule to get\n$-(1 + e^{-z})^{-2})(\\frac{d}{z} 1 + e^{-z})$. Which equals:\n\n$-(1 + e^{-z})^{-2})(\\frac{d}{dz}(1 + e^{-z})) = -(1 + e^{-z})^{-2})(-1)(e^{-z}))$\n\nAnd:\n$\\frac{e^{-z}}{(1 + e^{-z})^{2}}$. \n\nIf you do a lot of algebra there, you can also write this as $\\sigma(x) \\cdot (1 - \\sigma(x))$.\nThat is more easily seen if you work backwards.\n\n$\\frac{1}{1 + e^{-z}}(1 - \\frac{1}{1 + e^{-z}}) = \\frac{1}{1 + e^{-z}} - \\frac{1}{( 1 + e^{-z})^2} = \\frac{1 + e^{-z} - 1}{( 1 + e^{-z})^2}$\n\nAnyway, we have $(-1)(y - \\hat{y}) \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}} \\cdot (x_2)$. The point is that the big three differentiations here are not very hard to do.\n\nTo differentiate the other weights, we do all the same steps, but we treat w2 as a constant.\n\nWe start out the same until we get to: $(y - \\hat{y}) \\cdot \\sigma'(w2 * x) \\cdot (w2 * x)'$. Here, we treat $w2$ as the constant, but $x$ depends on $w1$, which is what we are interested in now.\n\nHence the derivative now is $w2 \\cdot x'$, which is $\\sigma(w1 * x_1)$ (the $x_1$ is the $x$ that w1 sees, not the one that w2 sees after going through w1 and $\\sigma$).\n\nDifferentiating $\\sigma(w1 * x_1)$ works out the same as last time, just with a different $z$ this time.\n\nOur final derivative now is $(-1)(y - \\hat{y}) \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}} \\cdot (w2) \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}} \\cdot x_1$\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Group Them\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The math is actually pretty simple, if you understand the chain rule, product or quotient rule, and the gist of partial derivatives.\n\nWe can get a little more insight into what the calculus is doing if we group things together. For instance, $(y - \\hat{y})$ is the error, and $\\frac{e^{-z}}{(1 + e^{-z})^{2}}$ holds w2 multipled by x (in the z) and hence shows us how much of a mistake this neuron has made.\n\nWe put them under a single notation, called \"the back-propagating error, $\\delta^{(3)}$.\"\n\nFor w1, we have to multiply again by its own $\\frac{e^{-z}}{(1 + e^{-z})^{2}}$. We multiply each error by the size of the neuron, and of its child neuron!\n\nI suppose this means earlier weights get adjusted more; they are responsible for the faults of their children!\n\nWhen we combine $\\delta^{(3)}$ with that whole $w2 \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}}$ bit we call it $\\delta^{(2)}$.\n\nTo make this all work out with many examples, you have to flip and spin matrices a bit.\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}