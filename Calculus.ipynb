{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Sum Rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof of (d/dx) [f(x) + g(x)] = (d/dx) f(x) + (d/dx) g(x) from the definition.\n",
    "\n",
    "We can use the definition of the derivative:\n",
    "\n",
    "$\\frac{d}{dx} f(x) =\t\\lim_{x\\to 0} \\frac{f(x+d)-f(x)}{d}$\n",
    "\n",
    "Therefore, $\\frac{d}{dx} [f(x) + g(x)] =\t\\lim_{x\\to 0} \\frac{[f(x+d)+ g(x+d)] - [f(x)+g(x)]}{d}$\n",
    "\n",
    "Let's just rearrange this a little: $\\frac{[f(x+d) - f(x) ] + [g(x+d) + g(x)]}{d}$, and then $\\frac{f(x+d) - f(x)}{d} + \\frac{g(x+d) + g(x)}{d}$.\n",
    "\n",
    "But this is just our original definition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Chain Rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prime notation shows the mechanics clearer, Leibniz notation the concept.\n",
    "Prime notation: $(f o g)' (x) = f'(g(x)) \\cdot g'(x)$\n",
    "\n",
    "And from Leibniz: $\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}$\n",
    "\n",
    "Since we are asking about a rate of change, relative to another rate of change,\n",
    "which is relative to x, it make sense that any change to x gets amplified as it\n",
    "ricochets through our little machine.\n",
    "\n",
    "dy/dx is the result of a function. It is f(x + dx) - f(x) by (x + dx).\n",
    "Actually, I suppose they are both infinitesimals, but the dy infinitesimal is\n",
    "obtained through a function.\n",
    "\n",
    "The proof is a straight-up algebraic \"chain\" of reasoning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember $\\Delta y = f(a + \\Delta x) - f(x)$, and as $\\Delta x \\rightarrow 0$ we have $\\Delta y/\\Delta x = f'(a)$.\n",
    "\n",
    "Let's make $\\epsilon$ equal the difference between our [ difference quotient ](https://en.wikipedia.org/wiki/Difference_quotient) and the derivative.\n",
    "\n",
    "As $\\Delta x \\rightarrow 0$, we have $\\epsilon = (\\frac{\\Delta y}{\\Delta x} - f'(a)) = f'(a) - f'(a) = 0$.\n",
    "\n",
    "This lets us do $\\epsilon = (\\Delta y/\\Delta x - f'(a) \\Rightarrow \\Delta y = f'(a) \\Delta x + \\epsilon \\Delta x$\n",
    "\n",
    "To make $\\epsilon$ differentiable, we let it go to 0 when $\\Delta x = 0$.\n",
    "$\\Delta y = f'(a) \\Delta x + \\epsilon \\Delta x$ where $\\epsilon \\to 0$ as $\\Delta x \\to 0$.\n",
    "\n",
    "This is a little obscurantist, because f'(a) and our difference quotient are exactly\n",
    "the same thing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stage is now set. Suppose $u = g(x)$ is differentiable at a, and $y = f(u)$ at \n",
    "the same location, but we'll say $b = g(a)$. $f(g(a))$, or $f(b)$.\n",
    "\n",
    "Then, $\\Delta u = g'(a) \\Delta x + \\epsilon \\Delta x$. And this equals $[g'(a) + \\epsilon](\\Delta x)$.\n",
    "\n",
    "And, $\\Delta y = f'(b) (\\Delta u) + \\epsilon (\\Delta u)$. And, $= [f'(b) + \\epsilon](\\Delta u)$.\n",
    "\n",
    "Substituting the first result into the second, we have\n",
    "$\\Delta y = [f'(b) + \\epsilon] [g'(a) + \\epsilon](\\Delta x)$.\n",
    "Now as $\\Delta x \\to 0$, so does $\\Delta u$, and thus both our $\\epsilon \\to 0$.\n",
    "Therefore $\\frac{dy}{dx} = (\\Delta x \\to 0) \\frac{(\\Delta y)}{(\\Delta x)} = (lim_{\\Delta x \\to 0}) [f'(b) + \\epsilon] [g'(a) + \\epsilon] = f'(b) g'(a) = f'(g(a)) g'(a)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This proves the Chain Rule.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a way to represent the change in y as the result of the derivative of\n",
    "a point plus a small change, multiplied by the change in x - to show a change in\n",
    "y in reference to an already-established rate.\n",
    "\n",
    "This somewhat sneaky characterization let us see more clearly what we could not\n",
    "before - that the change in y relative to the change in x, when functions are composed,\n",
    "depends on the derivatives of both functions.\n",
    "\n",
    "$\\epsilon = \\frac{\\Delta y)}{\\Delta x} - \\frac{\\Delta y}{\\Delta x}$\n",
    "\n",
    "$\\Delta x \\epsilon = \\Delta y - (\\Delta x)[\\frac{(\\Delta y}{(\\Delta x)}]$\n",
    "\n",
    "$\\Delta y = (\\Delta x)[\\frac{(\\Delta y}{(\\Delta x)}] + (\\Delta x) \\epsilon$\n",
    "\n",
    "$\\Delta y = (\\Delta x)[\\frac{(\\Delta y}{(\\Delta x)} + \\epsilon]$\n",
    "\n",
    "$(\\frac{\\Delta y)}{(\\Delta x)} = (\\frac{\\Delta y)}{(\\Delta u)} \\frac{(\\Delta u}{(\\Delta x)}$\n",
    "\n",
    "But since we are not using infinitesimals, and we use the Leibniz notation because \n",
    "sometimes it shows these relations more clearly, we cannot cancel out $(\\Delta u)$.\n",
    "Rather, these represent the results of taking the limit, and given any actual\n",
    "functions f and g, we would have to apply our usual method of adding an h and\n",
    "doing algebra, and so on.\n",
    "\n",
    "So we can treat $\\Delta x$ and others above as algebraic quantities, but to actually\n",
    "find the derivative we then must find the limit as $\\Delta x \\to 0$. We can work with them\n",
    "algebraically before taking the limit, but not use them to actually find the derivative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sal Khan's Method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is, naturally, the easiest.\n",
    "\n",
    "$\\frac{dy}{dx} = lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x}$ \n",
    "\n",
    "If we multiply the last part, we get:\n",
    "$lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x} \\frac{\\Delta u}{\\Delta u} = lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta u} \\frac{\\Delta u}{\\Delta x}$ \n",
    "\n",
    "But we're allowed to take the limits separately.\n",
    "\n",
    "$lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta u} \\frac{\\Delta u}{\\Delta x} = lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta u} \\cdot lim_{\\Delta x \\to 0} \\frac{\\Delta u}{\\Delta x}$ \n",
    "\n",
    "And we can change the first x to a u, because as x approaches 0, so does the function of x:\n",
    "\n",
    "$lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta u} \\frac{\\Delta u}{\\Delta x} = lim_{\\Delta u \\to 0} \\frac{\\Delta y}{\\Delta u} \\cdot lim_{\\Delta x \\to 0} \\frac{\\Delta u}{\\Delta x}$ \n",
    "\n",
    "And naturally that's $\\frac{dy}{du} \\frac{du}{dx}$.\n",
    "\n",
    "(although so far as I can see, when we say u approaches 0 when x does, we are kind of cheating, because we are saying u is 0 in the first but not in the second).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the video's explanation confusing, but the calculus is very simple.\n",
    "\n",
    "The chain rule is like using silverware at a fine dining experience: start from the outside and work in.\n",
    "\n",
    "For instance $x^2$ can be thought of as $(x)^2$. Take the derivative of the outer function, $2(x)$, then multiply by the derivative of the inner function, which is $x$ with a derivative of $1$: $2(x) * 1 = 2x$.\n",
    "\n",
    "\n",
    "\\begin{tabular}{ c c }\n",
    " Outer & x^4 \\\\ \n",
    "     & (x^2)^2 \\\\  \n",
    " to & 2 x^2 \\cdot (x^2)' \\\\\n",
    "    &  2 \\cdot 2x \\cdot x' \\\\\n",
    " Inner & 2 \\cdot 2x \\cdot x = 4x^3\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "To take a partial derivative, you take one variable at a time, and treat the other variables like constants. For instance, if you had $2x$, you could split it into $x + x$ and differentiate each separately.\n",
    "\n",
    "The partial derivative asks \"how much did each part contribute?\" Here each part contributed $1$.\n",
    "\n",
    "Our cost function is $J = \\sum \\frac{1}{2}(y - \\hat{y})^2$, and we want to take its derivative so we know how to improve our weights.\n",
    "\n",
    "We can add it all up later, so we ignore the summation. The derivative of $\\frac{1}{2}(y - \\hat{y})^2$ is just $ 2 \\* \\frac{1}{2}(y - \\hat{y})$ times the derivative of the inner function.\n",
    "\n",
    "Clearly the 2 and 0.5 multiply out, leaving us with $(y - \\hat{y})$ for the first outer function.\n",
    "\n",
    "For the inner function, $y$ is a constant and disappears. If we separate the negative sign like $(-1) \\hat{y}$, it won't disappear but the math is easier.\n",
    "\n",
    "To differentiate $\\hat{y}$, we look at the actual function, which is $\\sigma(w2 * x)$, where $x$ is the example that W2 gets. So we differentiate $\\sigma$, then differentiate the inner function. Let's differentiate $\\sigma$ in a second or two.\n",
    "\n",
    "At this point our entire derivative is just: $(y - \\hat{y}) \\cdot \\sigma'(w2 * x) \\cdot (w2 * x)'$, where we haven't yet figured out the last term.\n",
    "\n",
    "The last term is easy though. $w2$ is our variable, and $x$ is a constant. It is like normal calculus, where you have to differentiate $2x$ or something, and the derivative is just $2$.\n",
    "\n",
    "Here the derivative is $x$, which is the result of $\\sigma(w1 * x)$, which is more math to do, but we don't have to do any calculus on it, it is just straight multiplication.\n",
    "\n",
    "We now have $(y - \\hat{y}) \\cdot \\sigma'(w2 * x) \\cdot x$, and all we have to do is differentiate $\\sigma$.\n",
    "\n",
    "You can apply the product rule or the quotient rule, whichever you like.\n",
    "\n",
    "To take this derivative, rewrite as $(1 + e^{-z})^{-1}$, and apply the power\n",
    "rule to get $-(1 + e^{-z})^{-2}$, but then we must apply the product rule to get\n",
    "$-(1 + e^{-z})^{-2})(\\frac{d}{z} 1 + e^{-z})$. Which equals:\n",
    "\n",
    "$-(1 + e^{-z})^{-2})(\\frac{d}{dz}(1 + e^{-z})) = -(1 + e^{-z})^{-2})(-1)(e^{-z}))$\n",
    "\n",
    "And:\n",
    "$\\frac{e^{-z}}{(1 + e^{-z})^{2}}$. \n",
    "\n",
    "If you do a lot of algebra there, you can also write this as $\\sigma(x) \\cdot (1 - \\sigma(x))$.\n",
    "That is more easily seen if you work backwards.\n",
    "\n",
    "$\\frac{1}{1 + e^{-z}}(1 - \\frac{1}{1 + e^{-z}}) = \\frac{1}{1 + e^{-z}} - \\frac{1}{( 1 + e^{-z})^2} = \\frac{1 + e^{-z} - 1}{( 1 + e^{-z})^2}$\n",
    "\n",
    "Anyway, we have $(-1)(y - \\hat{y}) \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}} \\cdot (x_2)$. The point is that the big three differentiations here are not very hard to do.\n",
    "\n",
    "To differentiate the other weights, we do all the same steps, but we treat w2 as a constant.\n",
    "\n",
    "We start out the same until we get to: $(y - \\hat{y}) \\cdot \\sigma'(w2 * x) \\cdot (w2 * x)'$. Here, we treat $w2$ as the constant, but $x$ depends on $w1$, which is what we are interested in now.\n",
    "\n",
    "Hence the derivative now is $w2 \\cdot x'$, which is $\\sigma(w1 * x_1)$ (the $x_1$ is the $x$ that w1 sees, not the one that w2 sees after going through w1 and $\\sigma$).\n",
    "\n",
    "Differentiating $\\sigma(w1 * x_1)$ works out the same as last time, just with a different $z$ this time.\n",
    "\n",
    "Our final derivative now is $(-1)(y - \\hat{y}) \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}} \\cdot (w2) \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}} \\cdot x_1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math is actually pretty simple, if you understand the chain rule, product or quotient rule, and the gist of partial derivatives.\n",
    "\n",
    "We can get a little more insight into what the calculus is doing if we group things together. For instance, $(y - \\hat{y})$ is the error, and $\\frac{e^{-z}}{(1 + e^{-z})^{2}}$ holds w2 multipled by x (in the z) and hence shows us how much of a mistake this neuron has made.\n",
    "\n",
    "We put them under a single notation, called \"the back-propagating error, $\\delta^{(3)}$.\"\n",
    "\n",
    "For w1, we have to multiply again by its own $\\frac{e^{-z}}{(1 + e^{-z})^{2}}$. We multiply each error by the size of the neuron, and of its child neuron!\n",
    "\n",
    "I suppose this means earlier weights get adjusted more; they are responsible for the faults of their children!\n",
    "\n",
    "When we combine $\\delta^{(3)}$ with that whole $w2 \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}}$ bit we call it $\\delta^{(2)}$.\n",
    "\n",
    "To make this all work out with many examples, you have to flip and spin matrices a bit.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
