{"cells":[{"cell_type":"markdown","metadata":{},"source":[":header-kernel: python3\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part 1, Forward Propagation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We start off by defining some hyperparameters, constants which establish the\nstructure and behavior of the network and are not updated as we train.\n\nWe're going to look at hours of sleep and hours of study, and predict our test score.\n\nHere X is a 3x2 matrix, and Y is 3 x 1.\n\n$3W_{11} + 5W_{21}$ per each neuron is what we want. We need to figure out\nweights for each variable, for each example we have.\n\nWe can do this with matrix mulitplication (always remember [http://matrixmultiplication.xyz/](http://matrixmultiplication.xyz/)).\n\n$$  \\begin{bmatrix}\n3 & 5 \\\\\n5 & 1 \\\\\n10 & 2\n\\end{bmatrix} \\begin{bmatrix}\nW_{11} & W_{12} & W_{13}\\\\\nW_{21} & W_{22} & W_{33}\\\\\n\\end{bmatrix}\n$$\n\n$XW^{(1)} = Z^{(2)}$, where $Z^{(2)}$ is the activity of our second layer.\n\nSo each entry in Z is a sum of weighted inputs to each neuron. It has size 3x3:\none row for each example, one column for each hidden unit.\n\nNext we will independently apply the activation function to each entry in Z.\nWe'll use the sigmoid function, leaning on NumPy, which rather conveniently\napplies the function element-wise and returns the result with the same\ndimensions it was given.\n\nSo we have $a^{(2)} = f(Z^{(2)})$. We'll then apply one more set of weights to\nget our final output, and then run the activation function on that too.\n\n$Z^{(3)} = a^{(2)} W^{(2)}$, and $\\hat{y} = f(Z^{(3)})$\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[75, 82, 93]"}],"source":["X = [[3, 5], \n     [5, 1], \n     [10, 2]]\ny = [75, 82, 93]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"array([[0.37793827],\n       [0.48852971],\n       [0.4885244 ]])"}],"source":["import numpy as np\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\nclass Neural_Network(object):\n    def __init__(self):\n        self.inputLayersSize = 2\n        self.outputLayersSize = 1\n        self.hiddenLayersSize = 3\n\n        self.W1 = np.random.randn(self.inputLayersSize, self.hiddenLayersSize)\n        self.W2 = np.random.randn(self.hiddenLayersSize, self.outputLayersSize)\n\n    def forward(self, X):\n        Z2 = np.dot(X, self.W1)\n        a2 = self.sigmoid(Z2)\n\n        Z3 = np.dot(a2, self.W2)\n\n        y_hat = self.sigmoid(Z3)\n\n        return y_hat\n        \n    def sigmoid(self, z):\n        return 1/(1 + np.exp(-z))\n\nsigmoid(1), sigmoid(np.array([-1, 0, 1])), sigmoid(np.random.randn(3, 3))\n\nNN = Neural_Network()\ny_hat = NN.forward(X)\ny_hat"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}