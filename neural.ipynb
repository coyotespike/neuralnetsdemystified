{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Part 1, Forward Propagation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We start off by defining some hyperparameters, constants which establish the\nstructure and behavior of the network and are not updated as we train.\n\nWe're going to look at hours of sleep and hours of study, and predict our test score.\n\nHere X is a 3x2 matrix, and Y is 3 x 1.\n\n$3W_{11} + 5W_{21}$ per each neuron is what we want. We need to figure out\nweights for each variable, for each example we have.\n\nWe can do this with matrix multiplication (always remember [http://matrixmultiplication.xyz/](http://matrixmultiplication.xyz/)).\n\n$$  \\begin{bmatrix}\n3 & 5 \\\\\n5 & 1 \\\\\n10 & 2\n\\end{bmatrix} \\begin{bmatrix}\nW_{11} & W_{12} & W_{13}\\\\\nW_{21} & W_{22} & W_{23}\\\\\n\\end{bmatrix}\n$$\n\n$XW^{(1)} = Z^{(2)}$, where $Z^{(2)}$ is the activity of our second layer.\n\n$$\nZ^{(2)} = \\begin{bmatrix}\n3W_{11} + 5W_{21} & 3W_{12} + 5W_{22} & 3W_{13} + 5W_{33} \\\\\n5W_{11} + 1W_{21} & 5W_{12} + 1W_{22} & 5W_{13} + 1W_{33} \\\\\n10W_{11} + 2W_{21} & 10W_{12} + 2W_{22} & 10W_{13} + 2W_{33} \\\\\n\\end{bmatrix}\n$$\n\nSo each entry in Z is a sum of weighted inputs to each neuron. It has size 3x3:\none row for each example, one column for each hidden unit.\n\nNext we will independently apply the activation function to each entry in Z.\nWe'll use the sigmoid function, leaning on NumPy, which rather conveniently\napplies the function element-wise and returns the result with the same\ndimensions it was given.\n\n$$\na^{(2)} = \\begin{bmatrix}\n\\sigma(3W_{11} + 5W_{21}) & \\sigma(3W_{12} + 5W_{22}) & \\sigma(3W_{13} + 5W_{33}) \\\\\n\\sigma(5W_{11} + 1W_{21}) & \\sigma(5W_{12} + 1W_{22}) & \\sigma(5W_{13} + 1W_{33}) \\\\\n\\sigma(10W_{11} + 2W_{21}) & \\sigma(10W_{12} + 2W_{22}) & \\sigma(10W_{13} + 2W_{33}) \\\\\n\\end{bmatrix}\n$$\n\nSo we have $a^{(2)} = f(Z^{(2)})$. We'll then apply one more set of weights to\nget our final output, with dimensions 3 x 1, and then run the activation function on that too.\n\n$Z^{(3)} = a^{(2)} W^{(2)}$, and $\\hat{y} = f(Z^{(3)})$\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"| 75 | 82 | 93 |"}],"source":["X = [[3, 5], \n     [5, 1], \n     [10, 2]]\ny = [75, 82, 93]\ny"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"array([[0.76854999],\n       [0.81874552],\n       [0.83537981]])"}],"source":["import numpy as np\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\nclass Neural_Network(object):\n    def __init__(self):\n        self.inputLayersSize = 2\n        self.outputLayersSize = 1\n        self.hiddenLayersSize = 3\n\n        self.W1 = np.random.randn(self.inputLayersSize, self.hiddenLayersSize)\n        self.W2 = np.random.randn(self.hiddenLayersSize, self.outputLayersSize)\n\n    def forward(self, X):\n        self.Z2 = np.dot(X, self.W1)\n        self.a2 = self.sigmoid(self.Z2)\n\n        self.Z3 = np.dot(self.a2, self.W2)\n\n        y_hat = self.sigmoid(self.Z3)\n\n        return y_hat\n        \n    def sigmoid(self, z):\n        return 1/(1 + np.exp(-z))\n\nsigmoid(1), sigmoid(np.array([-1, 0, 1])), sigmoid(np.random.randn(3, 3))\n\nNN = Neural_Network()\ny_hat = NN.forward(X)\ny_hat"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, these results are completely terrible compared to our actual test\nscores! That is because we are using random weights. Next we need to update our weights.\n\nOur goal now is to quantify how wrong our predictions are, figure out how to\nupdate our weights in the right direction, and use our wrongess-quantity to\nadjust the weights by some suitable amount.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Quantifying wrongness with a loss function\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We could use several measures of wrongness. For instance, we could just take\n$y - \\hat{y}$, and that would be fine. Because we're coming up with a single\nweight for all our examples, we would average this. Our loss function would thus\nbe Mean Absolute Error, or MAE.\n\nHowever, Mean Squared Error is more commonly used, although\n[arguments for this are not perfectly convincing](https://stats.stackexchange.com/questions/470626/why-is-using-squared-error-the-standard-when-absolute-error-is-more-relevant-to). It is a little easier to do\ncalculus on, and most importantly, because it is a convex function, we can be\nsure it will be defined at 0, and more extreme errors will get penalized more,\nmeaning we will learn faster the wronger we are, which is nice.\n\nMean Squared Error looks like $\\sum (y - \\hat{y})^2$, and if we divide by 2 to\nhelp us do calculus later it will still have all the properties we want: $J = \\sum \\frac{1}{2}\n(y - \\hat{y})^2$.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Which way to jiggle the weights\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have a loss function, how do we know how to improve our weights?\nRandom guessing will not work, due to the curse of dimensionality. With 6\nweights, assume they might have 1000 possible values each, and we have\n$1000^{6}$ guesses to make!\n\nWe could also jiggle each weight up or down a little, and see if the whole thing\nimproves. That will work but be slow.\n\nFortunately we have a better way! We want to know how our cost function J\nchanges when $\\hat{y}$, or W, changes. This is a derivative! If the derivative\nis positive, we are heading in the wrong direction. We'll keep changing until\nthe derivative of our loss function starts getting worse again.\n\nIf we consider one weight at a time, then we want to know how J changes when\njust one weight changes, and that's a partial derivative: $\\frac{\\partial\nJ}{\\partial W}$.\n\nThis is one reason we chose our loss function as we did. It's convex, so we will\nalways know which direction to go in. In higher dimensions, the combination of\nall these weights could get us stuck in a local minimum, but if we update our\nweights one at a time (stochastic gradient descent), we might be fine anyway.\n\nAnyway, today we will do batch gradient descent, and update them all at once,\nbased on the partial derivative of each.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Doing the Math\n\n"]},{"cell_type":"markdown","metadata":{},"source":["For $W^{(1)}$ we should get:\n\\\\[\n\n\\begin{bmatrix}\n\\frac{\\partial J}{\\partial W_{11}} & \\frac{\\partial J}{\\partial W_{12}} & \\frac{\\partial J}{\\partial W_{13}}\\\\\n\\frac{\\partial J}{\\partial W_{21}} & \\frac{\\partial J}{\\partial W_{22}} & \\frac{\\partial J}{\\partial W_{33}}\\\\\n\\end{bmatrix}\n\n\\\\]\n\nAnd for $W^{(2)}$ we should get:\n\\\\[\n\n\\begin{bmatrix}\n\\frac{\\partial J}{\\partial W_{11}^{(2)}} \\\\\n\\frac{\\partial J}{\\partial W_{21}^{(2)}} \\\\\n\\frac{\\partial J}{\\partial W_{31}^{(2)}}\n\\end{bmatrix}\n\n\\\\]\n\n$\\frac{\\partial J}{\\partial W^{(2)}} = \\frac{\\partial \\sum \\frac{1}{2} (y -\n\\hat{y})^2}{\\partial W^{(2)}}$, since that's what J is.\n\nThe sum here is adding the error from each example to create an overall cost.\nThe Sum Rule, $\\frac{d}{dx}(u + v) = \\frac{du}{dx} + \\frac{dv}{dx}$, says that\nwe can move the summation outside our derivative, which is handy.\n\n$\\sum \\frac{\\partial \\frac{1}{2} (y - \\hat{y})^2}{\\partial W^{(2)}}$\n\nWe'll come back and add up later.\n\nWell, next we apply the power rule: $\\frac{\\partial J}{\\partial W^{(2)}} = 2\n\\cdot \\frac{1}{2} (y - \\hat{y})} = (y - \\hat{y})$, and isn't that convenient.\n\nThat was the outer function, now to follow the chain rule we must take the\nderivative of the inner function.\n\nA better name for back propagation might be, don't stop doing the chain rule, ever!\n\nThe $y$ is a constant and goes to 0.\n\n$\\frac{\\partial J}{\\partial W^{(2)}} = (y - \\hat{y}) \\cdot - \\frac{\\partial \\hat{y}}{\\partial W^{(2)}}$\n\nHowever $\\hat{y}$ is itself a function, $\\hat{y} = f(z^{(3)})$, and we must apply the chain rule again.\n\n$\\frac{\\partial \\hat{y}}{\\partial W^{(2)}} = \\frac{\\partial \\hat{y}}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}}$\n\nMeaning, again, we want the entire phrase:  \n$- (y - \\hat{y}) \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}}$\n\nNow, $f(z^{(3)})$ was our sigmoid function, which is $\\frac{1}{1 + e^{-z}}$.\n\nTo take this derivative, rewrite as $(1 + e^{-z})^{-1}$, and apply the power\nrule to get $-(1 + e^{-z})^{-2}$, but then we must apply the product rule to get\n$-(1 + e^{-z})^{-2})(\\frac{d}{z} 1 + e^{-z})$. Which equals:\n\n$-(1 + e^{-z})^{-2})(\\frac{d}{dz}(1 + e^{-z})) = -(1 + e^{-z})^{-2})(-1)(e^{-z}))$\n\nAnd:\n$\\frac{e^{-z}}{(1 + e^{-z})^{2}}$. If you do a lot of algebra there, you can\nalso write this as $\\sigma(x) \\cdot (1 - \\sigma(x))$.\n\nHaving found $f'(z^{(3)}$, we can slot that in.\n\n$- (y - \\hat{y}) \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}}$\n\nAnd we just need our last term. This is how our output - the sum of the\nactivated (first weights \\* inputs) multiplied by the second weights - changes as\nthe second weights change.\n\nHere's the thing, z3 is a linear function with respect to the W2 weights. If\nthey go up, the values go up. And the derivative of a linear function - it's\njust a plain ol' slope like we learned in 5th grade. Here the slope is the\nactivations, a2.\n\nOur final formula:\n\n$- (y - \\hat{y}) \\cdot \\frac{e^{-z}}{(1 + e^{-z})^{2}} \\cdot a^{(2)}$\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Zoom Out a Bit\n\n"]},{"cell_type":"markdown","metadata":{},"source":["All that calculus! We wanted to figure out how much our output depended on the\nfinal set of weights we applied. First we had to apply the power rule. Then, we\nfigured out how much our output was changed by our sigmoid function. Then we\nmultiplied by the activations.\n\nRemember we'll calculate all this by neuron, by weight. You can think about this\nas: we multiply by each activation, because that is how much they each\nproportionately contributed to the error. Crucially, this is what lets back\npropagation work its magic!\n\n**Our final formula in a nutshell**: *Multiply the size of error, by the derivative of the activation\nfunction, by all our examples with the weights and activation function applied.* \n\nIf you do all that, you will know just how to change each weight.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Multiply it out.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["$$  \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\end{bmatrix} - \\begin{bmatrix}\n\\hat{y}_1 \\\\\n\\hat{y}_2 \\\\\n\\hat{y}_3 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\ny_1 - \\hat{y}_1 \\\\\ny_2 - \\hat{y}_2 \\\\\ny_3 - \\hat{y}_3 \\\\\n\\end{bmatrix}\n$$\n\nWhen we applied sigmoid function, we also got a 3x1 matrix, and sigmoidPrime\nwill have the same shape. In other words $f'(z^{(3)}$ is also 3x1, and we can do\nelement-wise multiplication.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"array([[1],\n       [4],\n       [9]])"}],"source":["fakeYs = [[1], [2], [3]]\nfakeSigPrime = [[1], [2], [3]]\n\nnp.multiply(fakeYs, fakeSigPrime)"]},{"cell_type":"markdown","metadata":{},"source":["\\\\[\n\n\\begin{bmatrix}\ny_1 - \\hat{y}_1 \\\\\ny_2 - \\hat{y}_2 \\\\\ny_3 - \\hat{y}_3 \\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nf'(z^{(3)}_1) \\\\\nf'(z^{(3)}_2) \\\\\nf'(z^{(3)}_3) \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n\\delta^{(3)}_1 \\\\\n\\delta^{(3)}_2 \\\\\n\\delta^{(3)}_3 \\\\\n\\end{bmatrix} = \\delta^{(3)}\n\\]\n\nThis is called \"the back-propagating error, $\\delta^{(3)}$.\"\n\nAt this point we want to multiply by $a^{(2)}$, $\\delta^{(3)} a^{(2)}$. However,\nwe've got\n\n\\[\na^{(2)} = \n\\begin{bmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\\\\\n\\end{bmatrix}\n\n\\\\]\n\n,and these matrices don't match. You can't multiple 3x1 with 3x3.\n\nWe can make it work by transposing and multiplying, which I'll assume is the\nsame thing in linear algebra, or something. You can multiply 3x3 with 3x1.\n\n\\\\[\n\n\\begin{bmatrix}\na_{11} & a_{21} & a_{31}\\\\\na_{12} & a_{22} & a_{32}\\\\\na_{13} & a_{23} & a_{33}\\\\\n\\end{bmatrix} \\begin{bmatrix}\n\\delta^{(3)}_1 \\\\\n\\delta^{(3)}_2 \\\\\n\\delta^{(3)}_3 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\na_{11} ~ \\delta^{(3)}_1 + a_{21} ~ \\delta^{(3)}_2 + a_{31} ~ \\delta^{(3)}_3 \\\\\na_{12} ~ \\delta^{(3)}_1 + a_{22} ~ \\delta^{(3)}_2 + a_{32} ~ \\delta^{(3)}_3 \\\\\na_{13} ~ \\delta^{(3)}_1 + a_{23} ~ \\delta^{(3)}_2 + a_{33} ~ \\delta^{(3)}_3 \\\\\n\\end{bmatrix} \n\n\\\\]\n\nAnd the cool thing here is that the matrix multiplication is adding up across\nour examples - there's that summation $/Delta$ we took out earlier!\n\nYou can also think of batching gradient as contributing to the overall cost.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part 4\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"# [goto error]\n#+begin_example\n\n  NameErrorTraceback (most recent call last)\n  <ipython-input-32-d2ad5f72ac9b> in <module>\n       10     return decorator\n       11 \n  ---> 12 @add_method(Neural_Network)\n       13 def sigmoidPrime(self, z):\n       14   return np.exp(-z) / ((1 + np.exp(-z))**2)\n\n  <ipython-input-32-d2ad5f72ac9b> in decorator(func)\n        3 def add_method(cls):\n        4     def decorator(func):\n  ----> 5         @wraps(func)\n        6         def wrapper(self, *args, **kwargs):\n        7             return func(*args, **kwargs)\n\n  NameError: name 'wraps' is not defined\n#+end_example"}],"source":["# Magic from https://mgarod.medium.com/dynamically-add-a-method-to-a-class-in-python-c49204b85bd6\n# Makes it more convenient to add as I go in the same notebook\ndef add_method(cls):\n    def decorator(func):\n        @wraps(func) \n        def wrapper(self, *args, **kwargs): \n            return func(*args, **kwargs)\n        setattr(cls, func.__name__, wrapper)\n        return func\n    return decorator\n\n@add_method(Neural_Network)\ndef sigmoidPrime(self, z):\n  return np.exp(-z) / ((1 + np.exp(-z))**2)\n\n@add_method(Neural_Network)\ndef costFunctionPrime(self, X, y):\n    self.y_hat = self.forward(X)\n\n    self.sigPrime = self.sigmoidPrime(self.Z3)\n    self.wrongness = y - self.y_hat\n\n    self.delta_3 = np.multiply(-self.wrongness, self.sigPrime) # element-wise\n\n    dJdW2 = np.dot(self.a2, self.delta_3)\n\nNN.costFunctionPrime(x, y)"]},{"cell_type":"markdown","metadata":{},"source":["Where does X and y come from? Oh, we're building it a little wonky. I would\nfirst run, then call costFunctionPrime.\n\nWhy self.sigmoidPrime(self.Z3)?\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}